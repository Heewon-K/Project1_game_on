{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. metacritic url받아오기\n",
    "2. /critic-reviews 랑 /user-reviews 내용 가져오기 => url + '/critic-reviews' \n",
    "3. 가져 온 텍스트 영어인지 필터 : isalpha()사용? 아님 nltk이용\n",
    "4. 텍스트로 파일 저장\n",
    "==> wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 게임 리스트가 담긴 csv불러와서 검색에 용이하게 전처리!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../rank2023.csv')\n",
    "df = df.loc[0:99,:]\n",
    "\n",
    "game_list = []\n",
    "for i in df.Game:\n",
    "    game_name = i.replace(' ', '-').lower()\n",
    "    game_name = game_name.replace(\"'\", '')\n",
    "    # 소문자, 특수기호 빼기\n",
    "    game_list.append(game_name)\n",
    "    \n",
    "game_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### url이 스팀 웹에 없는 게임들이 있으므로 일일이 matacritic 웹사이트에서 검색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test 미니 데이터 돌려보기!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting driver\n",
    "path = '../driver/chromedriver.exe'\n",
    "driver = webdriver.Chrome(service = Service(path))\n",
    "\n",
    "driver.get('https://www.metacritic.com/game/pc/dave-the-diver/critic-reviews')\n",
    "\n",
    "def get_content(driver):\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    parent_div = soup.select('div.review_body')\n",
    "    # 본문 내용 가져오기\n",
    "    text = []\n",
    "    for n in range(len(parent_div)):\n",
    "        if not parent_div[n].find(class_ = 'blurb blurb_expanded') == None:\n",
    "            content = parent_div[n].find(class_ = 'blurb blurb_expanded').get_text()\n",
    "            if detect(content) == 'en':\n",
    "                text.append(content)\n",
    "            else:\n",
    "                pass          \n",
    "        else:\n",
    "            txt = parent_div[n].get_text()\n",
    "            if detect(txt) == 'en':\n",
    "                text.append(txt)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    # 만들어진 text리스트를 str으로 변환\n",
    "    textStr = ' '.join(map(str, text))\n",
    "        \n",
    "    # word cloud dictionary를 추출!\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.add('game')\n",
    "\n",
    "    wc = WordCloud(stopwords=stopwords, margin = 10, random_state=1).generate(textStr)\n",
    "    content = wc.words_\n",
    "        \n",
    "    return content\n",
    "\n",
    "get_content(driver)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다수의 게임정보를 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 결과 url을 만들어서 접속하기\n",
    "def critic_reviews(game_name):\n",
    "    url = 'https://www.metacritic.com/game/pc/' + game_name + '/critic-reviews'\n",
    "    # setting driver\n",
    "    return url\n",
    "\n",
    "def user_reviews(game_name):\n",
    "    url = 'https://www.metacritic.com/game/pc/' + game_name + '/user-reviews'\n",
    "    return url\n",
    "\n",
    "# critic reviews page에서 users reviews로 넘어가기\n",
    "def move_to_users():\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"main\"]/div[2]/ul/li[3]/span/span/a').click()\n",
    "\n",
    "# 게시물 내용 가져오기\n",
    "def get_content(driver):\n",
    "    # 현재 페이지 html정보 가져오기\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    parent_div = soup.select('div.review_body')\n",
    "    # 본문 내용 가져오기\n",
    "    text = []\n",
    "    for n in range(len(parent_div)):\n",
    "        if not parent_div[n].find(class_ = 'blurb blurb_expanded') == None:\n",
    "            # review가 길어서 collapse되어 있다면, expanded된 텍스트를 뽑아와야한다.\n",
    "            review = parent_div[n].find(class_ = 'blurb blurb_expanded').get_text()\n",
    "            if detect(review) == 'en':\n",
    "                text.append(review)\n",
    "            else:\n",
    "                pass          \n",
    "        else:\n",
    "            # expand 옵션이 없는 짧은 텍스트들은 그냥 리뷰를 받아오면 된다.\n",
    "            txt = parent_div[n].get_text()\n",
    "            if detect(txt) == 'en':\n",
    "                text.append(txt)\n",
    "            else:\n",
    "                pass\n",
    "    # 만들어진 text리스트를 str으로 변환\n",
    "    textStr = ' '.join(map(str, text))\n",
    "    \n",
    "    # word cloud dictionary 데이터를 추출!\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.add('game')\n",
    "\n",
    "    wc = WordCloud(stopwords=stopwords, margin = 10, random_state=1).generate(textStr)\n",
    "    content = wc.words_\n",
    "    \n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 만들어진 CLASS와 게임리스트로 결과 값(results)을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [1:06:20<00:00, 39.80s/it]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for game_name in tqdm(game_list):\n",
    "    try:\n",
    "        reviews= {}\n",
    "        path = '../driver/chromedriver.exe'\n",
    "        driver = webdriver.Chrome(service = Service(path))\n",
    "        \n",
    "        #critics reviews\n",
    "        driver.get(critic_reviews(game_name))\n",
    "        data = get_content(driver)\n",
    "        reviews['critics'] = data\n",
    "        # user reviews\n",
    "        move_to_users()\n",
    "        data = get_content(driver)\n",
    "        reviews['users'] = data   \n",
    "        \n",
    "        driver.close()\n",
    "        results[game_name] = reviews\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과값을 보고 추가할 stopwords가 있나 생각해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "for game_name in game_list:\n",
    "    try:\n",
    "        keys = results[game_name]['critics'].keys()\n",
    "        key_list = list(keys)\n",
    "        for i in key_list:\n",
    "            total.append(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords corpus (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Assuming you have the text stored in a variable called \"videogame_review\"\n",
    "# Replace \"videogame_review\" with your actual text data\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = total #nltk.word_tokenize(text)\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "filtered_tokens = [token.lower() for token in tokens if token.lower() not in stop_words and token.isalpha()]\n",
    "\n",
    "# Calculate the frequency distribution\n",
    "fdist = FreqDist(filtered_tokens)\n",
    "\n",
    "# Get the top 25 most frequent terms\n",
    "top_terms = fdist.most_common(25)\n",
    "\n",
    "# Print the top terms\n",
    "for term, frequency in top_terms:\n",
    "    print(term, frequency)\n",
    "\n",
    "# Plot the frequency distribution\n",
    "#fdist.plot(25, cumulative=False)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과값 json으로 저장!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path where you want to save the JSON data\n",
    "file_path = r'C:\\Users\\Playdata\\Desktop\\data.json'\n",
    "\n",
    "# Open the file in write mode and use json.dump() to write the dictionary data as JSON\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(results, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
